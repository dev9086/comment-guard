---
title: Comment Guard
emoji: ğŸ›¡ï¸
sdk: streamlit
app_file: app.py
pinned: false
---
# ğŸ›¡ï¸ Comment Guard - Toxic Comment Classifier

[![Open in Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Live_Demo-blue)](https://huggingface.co/spaces/Dev9893/comment-guard)
![Project Status](https://img.shields.io/badge/status-active_development-yellow)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

End-to-end solution to combat social media toxicity built from scratch without external APIs/pre-trained models.

```python
# Sample usage
from comment_guard import predict_toxicity

print(predict_toxicity("Your mother is trash!"))
# Output: {'toxic': 0.986, 'clean': 0.014}
```

## ğŸ” Why Comment Guard?
- 98.6% accuracy on toxicity classification
- 5x faster than API alternatives
- Culture-aware detection (500+ Hindi/English slurs)
- Zero external dependencies

## ğŸš§ Current Status (Active Development)
```diff
+ Core ML pipeline fully functional
! Deployment workflow being optimized
- Web interface not production-ready
```

## âš™ï¸ Installation
```bash
git clone https://github.com/dev9086/comment-guard.git
pip install -r requirements.txt
```

## ğŸ§ª Running the Model
```bash
python src/predict.py --text "Sample comment to classify"
```

## ğŸ“‚ Project Structure


## ğŸŒ Live Demo
Access the stable version on Hugging Face:  
[![Hugging Face Demo](https://img.shields.io/badge/ğŸ”—_Try_Live_Demo-FFD21F?style=for-the-badge)](https://huggingface.co/spaces/Dev9893/comment-guard)



## ğŸ“œ License
MIT License - See [LICENSE](LICENSE) for details
